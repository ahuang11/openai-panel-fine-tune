{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic dataset pipeline\n",
    "\n",
    "Synthetically create a dataset from expensive model outputs for fine-tuning cheap models.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1. [Create potential questions](#create-potential-questions) - Generating 20 usage questions about a package using `gpt-3.5-turbo`.\n",
    "2. [Answer the questions](#answer-the-questions) - Using `gpt-4-turbo-preview` to answer the questions and conducting a quick quality assessment.\n",
    "4. [Export to JSONL](#export-to-jsonl) - Exporting the dataset to a JSONL file to save progress.\n",
    "5. [Review the dataset](#review-the-dataset) - Reviewing the dataset and making necessary corrections using a Panel app.\n",
    "6. [Related questions](#related-questions) - Generating derived questions to the original questions to increase the size of the dataset.\n",
    "7. [Update the tone to how a user might ask](#update-the-tone) - Updating the tone of the questions to how a typical user might ask.\n",
    "8. [Multi-turn conversation](#multi-turn-conversation) - Creating a multi-turn conversation dataset by chaining the questions and answers together.\n",
    "9. [Fine-tune the model](#fine-tune-the-model) - Fine-tuning a model on the dataset using the OpenAI platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFRESH = False  # Set to False to prevent rerunning old cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create potential questions\n",
    "\n",
    "Using `gpt-3.5-turbo`, generate 20 usage questions about a package. These questions will be used to mimic the questions that a user might ask about a package, and form the basis of the synthetic dataset--later, we will be rephrasing these questions to create a larger dataset.\n",
    "\n",
    "Note, these questions can also be manually written by you, or actual user questions extracted from Discord/Discourse.\n",
    "\n",
    "We use `gpt-3.5-turbo` here because it is faster and we don't really need deep reasoning abilities to ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import marvin\n",
    "import asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "@marvin.fn(model_kwargs={\"model\": \"gpt-3.5-turbo\"})\n",
    "async def batch_ask(package: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Imagine you are a new user to the given `package`.\n",
    "    Ask at least 20 usage questions about the `package`\n",
    "    to help you get started.\n",
    "    \"\"\"\n",
    "\n",
    "if REFRESH:\n",
    "    package = \"HoloViz Panel\"\n",
    "    questions = await batch_ask(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer the questions\n",
    "\n",
    "Using `gpt-4-turbo-preview`, we will now answer the questions. These answers will be used as the ground truth for the synthetic dataset.\n",
    "\n",
    "In the pipeline below, we also have a quick quality assessment of the answer by using `eval` to ensure the code runs successfully and the answer is also reviewed by itself (`gpt-4-turbo-preview`) using a different prompt (different agent). The downside of using `eval` is that you must have all the packages it uses installed for it to work as intended. Later, we will create an app later to view the inputs and output.\n",
    "\n",
    "Note, you may also manually answer the questions for correctness, especially if the package is not inside GPT-4's training data, but because HoloViz Panel is already a part of the training data, we can use the model to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@marvin.fn(model_kwargs={\"model\": \"gpt-4-turbo-preview\", \"max_tokens\": 4096})\n",
    "async def expertly_answer(\n",
    "    package: str, question: str, critiques: str | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    You are a world expert on the `package`.\n",
    "\n",
    "    Please first concisely understand the `question`,\n",
    "    then provide a response to the `question` in an\n",
    "    applicable, useful manner that helps the user get unblocked.\n",
    "\n",
    "    If critiques is provided, please ensure that the critique\n",
    "    is addressed in the response, but do not mention it.\n",
    "\n",
    "    If a code snippet is needed, please produce a single MCVE\n",
    "    using best practices in backticks with the language specified;\n",
    "    it must be fully copy/pastable and runnable.\n",
    "    Everything should be made as simple as possible, but not simpler.\n",
    "    Try not to have multiple snippets in one answer.\n",
    "\n",
    "    Best practices include, using `pn.bind` instead of `param.watch`,\n",
    "    `pn.state.onload` function for slow loading components, `pn.cache` for\n",
    "    reusing computations, wrapping `hv.DynamicMap` around\n",
    "    `pn.bind` for preventing HoloViews' plot from resetting zoom,\n",
    "    Manage loading states effectively by encapsulating widget\n",
    "    activation within a try/finally block, using `param.Parameterized`\n",
    "    and `@pn.depends` for more involved applications,\n",
    "    prioritizing `panel_obj.servable()` instead of `pn.serve(panel_obj)`,\n",
    "    `template.show()` instead of `template.servable()`,\n",
    "    functions on the top, widgets, layout, bindings on the bottom,\n",
    "    and any other best practices that are relevant to the package.\n",
    "\n",
    "    Link to https://panel.holoviz.org/tutorials/index.html for tutorials,\n",
    "    https://panel.holoviz.org/reference/index.html for components / reference,\n",
    "    https://panel.holoviz.org/getting_started/index.html for getting started,\n",
    "    https://discourse.holoviz.org/ to ask other questions.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "\n",
    "    chain_of_thought: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise chain of thought when reviewing the answer\",\n",
    "    )\n",
    "\n",
    "    requires_revision: bool = Field(\n",
    "        ...,\n",
    "        description=\"Whether the answer requires revision\",\n",
    "    )\n",
    "\n",
    "\n",
    "@marvin.fn(model_kwargs={\"model\": \"gpt-4-turbo-preview\"})\n",
    "async def review_answer(package: str, question: str, answer: str) -> Review:\n",
    "    \"\"\"\n",
    "    You are a world-class reviewer of the `package`.\n",
    "    You check whether the `answer` is valid and factually correct.\n",
    "\n",
    "    \"\"\"\n",
    "    code_snippets = re.findall(r\"```.*?\\n(.*?)\\n```\", answer, re.DOTALL)\n",
    "\n",
    "    if code_snippets:\n",
    "        for code in code_snippets:\n",
    "            try:\n",
    "                exec(code)\n",
    "            except Exception as exc:\n",
    "                if \"No module named\" in str(exc):\n",
    "                    return\n",
    "\n",
    "                return Review(\n",
    "                    chain_of_thought=f\"Code snippet failed with error: {exc}\",\n",
    "                    requires_revision=True,\n",
    "                )\n",
    "\n",
    "\n",
    "async def pipe_question(package, question):\n",
    "    critiques = None\n",
    "    for _ in range(3):  # Retry up to 3 times\n",
    "        answer = await expertly_answer(package, question, critiques=critiques)\n",
    "        review = await review_answer(package, question, answer)\n",
    "        if not review.requires_revision:\n",
    "            return answer\n",
    "        critiques = f\"Answer requires revision: {review.chain_of_thought}\"\n",
    "    else:\n",
    "        print(f\"Failed to answer question: {question} due to {critiques}\")\n",
    "\n",
    "\n",
    "if REFRESH:  # toggle to False to prevent running\n",
    "    answers = await asyncio.gather(\n",
    "        *[pipe_question(package, question) for question in questions]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to JSONL\n",
    "\n",
    "Finally, we will export the dataset to a JSONL file to save our progress and later read it inside a Panel app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_text(text):\n",
    "    \"\"\"Escapes newline characters and double quotes in a given text.\"\"\"\n",
    "    return text.replace(\"\\n\", \"\\\\n\").replace('\"', '\\\\\"')\n",
    "\n",
    "def format_message(data_format, question, answer):\n",
    "    \"\"\"Formats a single message based on the data format.\"\"\"\n",
    "    escaped_question = escape_text(question)\n",
    "    escaped_answer = escape_text(answer)\n",
    "    \n",
    "    if data_format == \"openai\":\n",
    "        return f'{{\"messages\": [{{\"role\": \"system\", \"content\": \"You are a world class expert on {package}. Help the user get unblocked.\"}}, {{\"role\": \"user\", \"content\": \"{escaped_question}\"}}, {{\"role\": \"assistant\", \"content\": \"{escaped_answer}\"}}]}}\\n'\n",
    "    else:\n",
    "        return f'{{\"instruction\": \"You are a world class expert on {{package}}. Help the user get unblocked.\", \"input\": \"{escaped_question}\", \"output\": \"{escaped_answer}\"}}\\n'\n",
    "\n",
    "def write_dataset(filename, questions_answers, data_format, mode=\"w\"):\n",
    "    \"\"\"Writes the dataset to a file in the specified format.\"\"\"\n",
    "    with open(filename, mode) as f:\n",
    "        for question, answer in questions_answers:\n",
    "            if answer is None:\n",
    "                continue\n",
    "            message = format_message(data_format, question, answer)\n",
    "            f.write(message)\n",
    "\n",
    "if REFRESH:\n",
    "    data_format = \"openai\"\n",
    "    filename = f\"panel_dataset_{data_format}_base.jsonl\"\n",
    "    questions_answers = zip(questions, answers)\n",
    "\n",
    "    write_dataset(filename, questions_answers, data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the dataset\n",
    "\n",
    "Using a Panel app, we will review the dataset and make any necessary corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import json\n",
    "\n",
    "pn.extension(\"code_editor\", sizing_mode=\"stretch_both\")\n",
    "\n",
    "\n",
    "# Function to load Q&A data\n",
    "def load_qa_data(filepath):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return [json.loads(line) for line in file.read().splitlines()]\n",
    "\n",
    "\n",
    "# Function to save Q&A data\n",
    "def save_qa_data(data, filepath):\n",
    "    with open(filepath, \"w\") as file:\n",
    "        for item in data:\n",
    "            file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "# Function to update the displayed Q&A based on the selected index\n",
    "def update_display(index):\n",
    "    messages = qa_data[index][\"messages\"]\n",
    "    system_input.value = \"\"\n",
    "    user_input.value = \"\"\n",
    "    assistant_input.value = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system_input.value = message[\"content\"]\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            user_input.value = message[\"content\"]\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            assistant_input.value = message[\"content\"]\n",
    "    return pn.Column(system_input, user_input, assistant_input)\n",
    "\n",
    "\n",
    "# Save changes to the current Q&A\n",
    "def save_changes(event):\n",
    "    current_index = index_slider.value\n",
    "    qa_data[current_index][\"messages\"] = [\n",
    "        {\"role\": \"system\", \"content\": system_input.value},\n",
    "        {\"role\": \"user\", \"content\": user_input.value},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_input.value},\n",
    "    ]\n",
    "    save_qa_data(qa_data, filename)\n",
    "    print(\"Changes saved!\")\n",
    "\n",
    "\n",
    "async def update_answer(contents, user, instance):\n",
    "    try:\n",
    "        app_layout.loading = True\n",
    "        answer = await expertly_answer(\n",
    "            package,\n",
    "            user_input.value,\n",
    "            critiques=f\"Revise based on {contents!r}\\nTo revise:\\n'''\\n{assistant_input.value}\\n'''\",\n",
    "        )\n",
    "        assistant_input.value = answer\n",
    "    finally:\n",
    "        app_layout.loading = False\n",
    "\n",
    "\n",
    "if REFRESH:\n",
    "    # Load data\n",
    "    filename = \"panel_dataset_openai_added.jsonl\"\n",
    "    qa_data = load_qa_data(filename)\n",
    "\n",
    "    # UI Components\n",
    "    system_input = pn.widgets.TextAreaInput(\n",
    "        name=\"System Message\", max_length=1000, max_height=50\n",
    "    )\n",
    "    user_input = pn.widgets.TextAreaInput(\n",
    "        name=\"User Question\", max_length=1000, max_height=50\n",
    "    )\n",
    "    assistant_input = pn.widgets.CodeEditor(name=\"Assistant Answer\", language=\"python\")\n",
    "    save_button = pn.widgets.Button(\n",
    "        name=\"Save Changes\", button_type=\"success\", height=50, sizing_mode=\"stretch_width\"\n",
    "    )\n",
    "    index_slider = pn.widgets.IntSlider(\n",
    "        name=\"Message Index\",\n",
    "        start=0,\n",
    "        end=len(qa_data) - 1,\n",
    "        step=1,\n",
    "        sizing_mode=\"stretch_width\",\n",
    "    )\n",
    "\n",
    "\n",
    "    chat = pn.chat.ChatInterface(\n",
    "        callback=update_answer,\n",
    "        show_clear=False,\n",
    "        show_undo=False,\n",
    "        show_stop=False,\n",
    "        show_rerun=False,\n",
    "        show_avatar=False,\n",
    "        show_button_name=False,\n",
    "        width=450,\n",
    "        height=650,\n",
    "        help_text=\"Put your requests here to use AI to update the output.\"\n",
    "    )\n",
    "    save_button.on_click(save_changes)\n",
    "\n",
    "    # Layout\n",
    "    app_layout = pn.template.FastListTemplate(\n",
    "        sidebar=[chat],\n",
    "        main=[pn.Row(save_button, index_slider), pn.bind(update_display, index=index_slider)],\n",
    "        theme=\"dark\",\n",
    "        sidebar_width=450,\n",
    "    )\n",
    "\n",
    "    # Serve the app\n",
    "    app_layout.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related questions\n",
    "\n",
    "Generate derived questions to the original questions to increase the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not REFRESH:\n",
    "    data_format = \"openai\"\n",
    "    filename = f\"panel_dataset_{data_format}_base.jsonl\"\n",
    "    df = (\n",
    "        pd.read_json(filename, lines=True)\n",
    "        .explode(\"messages\")[\"messages\"]\n",
    "        .apply(pd.Series)\n",
    "        .query(\"role != 'system'\")\n",
    "    )\n",
    "    questions = df.query(\"role == 'user'\")[\"content\"].tolist()\n",
    "    answers = df.query(\"role == 'assistant'\")[\"content\"].tolist()\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "new_questions = await asyncio.gather(\n",
    "    *[\n",
    "        batch_ask(\n",
    "            f\"HoloViz Panel: what other details should I know about {question}\",\n",
    "        )\n",
    "        for question in questions\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_format = \"openai\"\n",
    "filename = f\"panel_dataset_{data_format}_added.jsonl\"\n",
    "questions_answers = zip(questions, answers)\n",
    "\n",
    "for n_questions in new_questions[1:]:\n",
    "    print(n_questions)\n",
    "    new_answers = await asyncio.gather(\n",
    "        *[pipe_question(package, question) for question in n_questions]\n",
    "    )\n",
    "    new_questions_answers = zip(n_questions, new_answers)\n",
    "    print(new_answers)\n",
    "    write_dataset(filename, new_questions_answers, data_format, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the tone to how a user might ask\n",
    "\n",
    "Much of the input queries are too formal. Let's update the tone to how a typical user might ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@marvin.ai_fn(model_kwargs={\"model\": \"gpt-4-turbo-preview\"})\n",
    "async def update_tone(question: str, tone: str) -> str:\n",
    "    \"\"\"\n",
    "    Update the tone of the given question to the new tone;\n",
    "    be sure to capture the essence of the tone, e.g. in\n",
    "    capitalization, punctuation, and word choice. Also,\n",
    "    make sure the updated question captures the original\n",
    "    meaning of the question.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "toned_filename = \"panel_dataset_openai_toned.jsonl\"\n",
    "for filename in [\"panel_dataset_openai_base.jsonl\", \"panel_dataset_openai_added.jsonl\"]:\n",
    "    for line in open(filename):\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(line)\n",
    "    df = (\n",
    "        pd.read_json(filename, lines=True)\n",
    "        .explode(\"messages\")[\"messages\"]\n",
    "        .apply(pd.Series)\n",
    "        .query(\"role != 'system'\")\n",
    "    )\n",
    "    questions = df.query(\"role == 'user'\")[\"content\"].tolist()\n",
    "    answers = df.query(\"role == 'assistant'\")[\"content\"].tolist()\n",
    "    toned_questions = await asyncio.gather(\n",
    "        *[\n",
    "            update_tone(\n",
    "                question, \"messages like a close friend; casual, informal, using lowercase\"\n",
    "            )\n",
    "            for question in questions\n",
    "        ]\n",
    "    )\n",
    "    toned_questions_answers = zip(toned_questions, answers)\n",
    "    write_dataset(toned_filename, toned_questions_answers, data_format, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn conversation\n",
    "\n",
    "Thus far, we focused on breadth, but now we will focus on depth. We will create a multi-turn conversation dataset by chaining the questions and answers together. This will allow us to fine-tune models that can handle multi-turn conversations.\n",
    "\n",
    "TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "When you have a large enough dataset (repeating the above steps as much as you like), you can fine-tune a model on it.\n",
    "\n",
    "You can use the OpenAI platform to fine-tune a model on your dataset by uploading the joined jsonl file: https://platform.openai.com/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"panel_dataset_openai_base.jsonl\",\n",
    "    \"panel_dataset_openai_added.jsonl\",\n",
    "    \"panel_dataset_openai_toned.jsonl\",\n",
    "]\n",
    "with open(\"panel_dataset_openai_joined.jsonl\", \"w\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, \"r\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marvin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
